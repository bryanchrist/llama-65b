  Running command git clone --quiet https://github.com/huggingface/transformers.git /tmp/pip-install-l2kq92oe/transformers_bcc16df31d4e4e45bc27226042bf1ce9
  Running command git clone --quiet https://github.com/huggingface/peft.git /tmp/pip-install-l2kq92oe/peft_e3699b8899ea43d2a1eae9cf07baa770
  Running command git clone --quiet https://github.com/huggingface/accelerate.git /tmp/pip-install-l2kq92oe/accelerate_9d1eda4088e74728be770495faf7dd0f
/home/brc4cb/.conda/envs/falcon_40B/lib/python3.9/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: Found duplicate ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] files: {PosixPath('/home/brc4cb/.conda/envs/falcon_40B/lib/libcudart.so.11.0'), PosixPath('/home/brc4cb/.conda/envs/falcon_40B/lib/libcudart.so')}.. We'll flip a coin and try one of these, in order to fail forward.
Either way, this might cause trouble in the future:
If you get `CUDA error: invalid device function` errors, the above might be the cause and the solution is to make sure only one ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] in the paths that we search based on your env.
  warn(msg)
Loading checkpoint shards:   0%|          | 0/14 [00:00<?, ?it/s]Loading checkpoint shards:   7%|▋         | 1/14 [00:43<09:21, 43.17s/it]Loading checkpoint shards:  14%|█▍        | 2/14 [01:26<08:35, 42.99s/it]Loading checkpoint shards:  21%|██▏       | 3/14 [02:07<07:44, 42.23s/it]Loading checkpoint shards:  29%|██▊       | 4/14 [02:43<06:36, 39.63s/it]Loading checkpoint shards:  36%|███▌      | 5/14 [03:20<05:49, 38.88s/it]Loading checkpoint shards:  43%|████▎     | 6/14 [03:52<04:53, 36.64s/it]Loading checkpoint shards:  50%|█████     | 7/14 [04:24<04:05, 35.07s/it]Loading checkpoint shards:  57%|█████▋    | 8/14 [04:53<03:18, 33.06s/it]Loading checkpoint shards:  64%|██████▍   | 9/14 [05:25<02:43, 32.64s/it]Loading checkpoint shards:  71%|███████▏  | 10/14 [05:58<02:11, 32.95s/it]Loading checkpoint shards:  79%|███████▊  | 11/14 [06:32<01:39, 33.19s/it]Loading checkpoint shards:  86%|████████▌ | 12/14 [07:09<01:08, 34.30s/it]Loading checkpoint shards:  93%|█████████▎| 13/14 [07:43<00:34, 34.39s/it]Loading checkpoint shards: 100%|██████████| 14/14 [07:57<00:00, 28.16s/it]Loading checkpoint shards: 100%|██████████| 14/14 [07:57<00:00, 34.12s/it]
'(ReadTimeoutError("HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)"), '(Request ID: 4b177d18-1048-4962-91c3-7fddb29cd972)')' thrown while requesting HEAD https://huggingface.co/huggyllama/llama-65b/resolve/main/tokenizer_config.json
/home/brc4cb/.conda/envs/falcon_40B/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:318: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
slurmstepd: error: *** JOB 51759810 ON udc-an37-1 CANCELLED AT 2023-07-28T13:56:15 DUE TO TIME LIMIT ***
